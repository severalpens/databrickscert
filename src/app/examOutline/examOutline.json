[
  {
    "section": 1,
    "title": "Databricks Lakehouse Platform",
    "topics": [
      {
        "topic": "Describe the relationship between the data lakehouse and the data warehouse.",
        "answer": "A data warehouse is a data repository that can be used by data analyst or data scientist. A data lake is a generic location where structured and unstructured data can be stored. A data lakehouse is a combination of the two, where structured data is stored in a data warehouse and unstructured data is stored in a data lake. The data lakehouse allows for the flexibility of a data lake while providing the structure and performance of a data warehouse."
      },
      {
        "topic": "Identify the improvement in data quality in the data lakehouse over the data lake.",
        "answer": "Data quality in the data lakehouse is improved over the data lake by providing a structured environment for data storage and processing. This includes features such as schema enforcement, ACID transactions, and data governance capabilities that ensure data integrity and consistency."
      },
      {
        "topic": "Compare and contrast silver and gold tables, which workloads will use a bronze table as a source, which workloads will use a gold table as a source.",
        "answer": "Bronze tables are typically used for raw, unprocessed data, serving as the initial landing zone for data ingestion. Silver tables are used for cleaned and transformed data, often serving as a source for more complex analytics. Gold tables are used for high-quality, aggregated data that is ready for reporting and business intelligence. Workloads that require raw data will use bronze tables, while workloads that require processed data will use silver or gold tables."
      },
      {
        "topic": "Identify elements of the Databricks Platform Architecture, such as what is located in the data plane versus the control plane and what resides in the customer’s cloud account",
        "answer": "The data plane is where the actual data processing occurs, including compute resources and storage. The control plane is responsible for managing the data processing tasks, including job scheduling, cluster management, and user access control. The customer’s cloud account typically contains the data storage and compute resources used by the Databricks platform."
      },
      {
        "topic": "Differentiate between all-purpose clusters and jobs clusters",
        "answer": "All-purpose clusters are designed for interactive data exploration and development, allowing users to run notebooks and perform ad-hoc queries. Jobs clusters are optimized for running scheduled jobs and batch processing tasks, providing a more efficient and cost-effective solution for automated workloads."
      },
      {
        "topic": "Identify how cluster software is versioned using the Databricks Runtime",
        "answer": "Databricks cluster software is versioned by the Databricks Runtime by assigning a specific version number to each release. This versioning allows users to select the appropriate runtime version for their clusters, ensuring compatibility with their workloads and access to the latest features and improvements."
      },
      {
        "topic": "Identify how clusters can be filtered to view those that are accessible by the user",
        "answer": "Clusters can be filtered by user access permissions, allowing users to view only those clusters they have permission to access. This can be done through the Databricks UI or API, where users can apply filters based on cluster status, owner, or other attributes."
      },
      {
        "topic": "Describe how clusters are terminated and the impact of terminating a cluster",
        "answer": "Clusters are terminated by stopping the cluster through the Databricks UI or API. Terminating a cluster releases the compute resources and storage associated with it, which can impact any running jobs or notebooks that were using the cluster. It is important to ensure that all work is saved before terminating a cluster to avoid data loss."
      },
      {
        "topic": "Identify a scenario in which restarting the cluster will be useful",
        "answer": "A scenario in which restarting the cluster will be useful is when there are performance issues or memory leaks in the cluster. Restarting the cluster can help clear any cached data, free up resources, and reset the environment to a clean state, improving overall performance."
      },
      {
        "topic": "Describe how to use multiple languages within the same notebook",
        "answer": "Multiple languages can be used within the same notebook by using language magic commands. For example, in a Databricks notebook, you can use `%python`, `%sql`, `%scala`, or `%r` at the beginning of a cell to specify the language for that cell. This allows users to mix and match languages as needed for their data processing tasks."
      },
      {
        "topic": "Identify how to run one notebook from within another notebook",
        "answer": "One notebook can be run from within another notebook using the `%run` magic command. This command allows users to include the content of another notebook, enabling code reuse and modularization of workflows. For example, `%run ./path/to/another_notebook` will execute the specified notebook and make its variables and functions available in the current notebook."
      },
      {
        "topic": "Identify how notebooks can be shared with others",
        "answer": "Notebooks can be shared with others by using the sharing options available in the Databricks UI. Users can share notebooks by providing access permissions to specific users or groups, exporting notebooks as HTML or IPython files, or publishing notebooks to a shared workspace. Additionally, notebooks can be versioned and managed through Databricks Repos for collaborative development."
      },
      {
        "topic": "Describe how Databricks Repos enables CI/CD workflows in Databricks",
        "answer": "Databricks Repos enables CI/CD workflows by integrating with Git repositories, allowing users to version control their notebooks and code. This integration supports branching, merging, and pull requests, enabling collaborative development and automated deployment of code changes. Users can also set up automated tests and deployments to ensure code quality and consistency across environments."
      },
      {
        "topic": "Identify Git operations available via Databricks Repos",
        "answer": "Git operations available via Databricks Repos include cloning repositories, committing changes, pushing and pulling updates, creating branches, merging branches, and resolving conflicts. These operations allow users to manage their code versions and collaborate effectively within the Databricks environment."
      },
      {
        "topic": "Identify limitations in Databricks Notebooks version control functionality relative to Repos.",
        "answer": "Databricks Notebooks version control functionality is limited compared to Repos in that it does not support advanced Git features such as branching, merging, and pull requests. Notebooks version control primarily tracks changes to the notebook content itself, while Repos provides a full Git experience with all the associated capabilities for collaborative development and version management."
      }
    ]
  },
  {
    "section": 2,
    "title": "ELT with Apache Spark",
    "topics": [
      {
        "topic": "Extract data from a single file and from a directory of files",
        "answer": "Here is an example of how to extract data from a single file and from a directory of files using PySpark:\n\n```python\n# Extracting data from a single file\nsingle_file_df = spark.read.format('csv').option('header', 'true').load('/path/to/single_file.csv')\n\n# Extracting data from a directory of files\ndirectory_df = spark.read.format('csv').option('header', 'true').load('/path/to/directory/*')\n```\nThis code reads CSV files, but you can change the format to JSON, Parquet, etc., as needed."
      },
      {
        "topic": "Identify the prefix included after the FROM keyword as the data type.",
        "answer": "The prefix included after the FROM keyword in a SQL query indicates the data source type, such as 'table', 'view', 'file', or 'directory'. For example, `FROM table_name` indicates a table, while `FROM '/path/to/file.csv'` indicates a file."
      },
      {
        "topic": "Create a view, a temporary view, and a CTE as a reference to a file",
        "answer": ""
      },
      {
        "topic": "Identify that tables from external sources are not Delta Lake tables.",
        "answer": ""
      },
      {
        "topic": "Create a table from a JDBC connection and from an external CSV file",
        "answer": ""
      },
      {
        "topic": "Identify how the count_if function and the count where x is null can be used",
        "answer": ""
      },
      {
        "topic": "Identify how the count(row) skips NULL values.",
        "answer": ""
      },
      {
        "topic": "Deduplicate rows from an existing Delta Lake table.",
        "answer": ""
      },
      {
        "topic": "Create a new table from an existing table while removing duplicate rows.",
        "answer": ""
      },
      { "topic": "Deduplicate a row based on specific columns.", "answer": "" },
      {
        "topic": "Validate that the primary key is unique across all rows.",
        "answer": ""
      },
      {
        "topic": "Validate that a field is associated with just one unique value in another field.",
        "answer": ""
      },
      {
        "topic": "Validate that a value is not present in a specific field.",
        "answer": ""
      },
      { "topic": "Cast a column to a timestamp.", "answer": "" },
      { "topic": "Extract calendar data from a timestamp.", "answer": "" },
      {
        "topic": "Extract a specific pattern from an existing string column.",
        "answer": ""
      },
      {
        "topic": "Utilize the dot syntax to extract nested data fields.",
        "answer": ""
      },
      {
        "topic": "Identify the benefits of using array functions.",
        "answer": ""
      },
      { "topic": "Parse JSON strings into structs.", "answer": "" },
      {
        "topic": "Identify which result will be returned based on a join query.",
        "answer": ""
      },
      {
        "topic": "Identify a scenario to use the explode function versus the flatten function",
        "answer": ""
      },
      {
        "topic": "Identify the PIVOT clause as a way to convert data from a long format to a wide format.",
        "answer": ""
      },
      { "topic": "Define a SQL UDF.", "answer": "" },
      { "topic": "Identify the location of a function.", "answer": "" },
      {
        "topic": "Describe the security model for sharing SQL UDFs.",
        "answer": ""
      },
      { "topic": "Use CASE/WHEN in SQL code.", "answer": "" },
      { "topic": "Leverage CASE/WHEN for custom control flow.", "answer": "" }
    ]
  },
  {
    "section": 3,
    "title": "Incremental Data Processing",
    "topics": [
      {
        "topic": "Identify where Delta Lake provides ACID transactions",
        "answer": ""
      },
      { "topic": "Identify the benefits of ACID transactions.", "answer": "" },
      {
        "topic": "Identify whether a transaction is ACID-compliant.",
        "answer": ""
      },
      { "topic": "Compare and contrast data and metadata.", "answer": "" },
      {
        "topic": "Compare and contrast managed and external tables.",
        "answer": ""
      },
      {
        "topic": "Identify a scenario to use an external table.",
        "answer": ""
      },
      { "topic": "Create a managed table.", "answer": "" },
      { "topic": "Identify the location of a table.", "answer": "" },
      {
        "topic": "Inspect the directory structure of Delta Lake files.",
        "answer": ""
      },
      {
        "topic": "Identify who has written previous versions of a table.",
        "answer": ""
      },
      { "topic": "Review a history of table transactions.", "answer": "" },
      { "topic": "Roll back a table to a previous version.", "answer": "" },
      {
        "topic": "Identify that a table can be rolled back to a previous version.",
        "answer": ""
      },
      { "topic": "Query a specific version of a table.", "answer": "" },
      {
        "topic": "Identify why Zordering is beneficial to Delta Lake tables.",
        "answer": ""
      },
      { "topic": "Identify how vacuum commits deletes.", "answer": "" },
      {
        "topic": "Identify the kind of files Optimize compacts.",
        "answer": ""
      },
      { "topic": "Identify CTAS as a solution.", "answer": "" },
      { "topic": "Create a generated column.", "answer": "" },
      { "topic": "Add a table comment.", "answer": "" },
      {
        "topic": "Use CREATE OR REPLACE TABLE and INSERT OVERWRITE",
        "answer": ""
      },
      {
        "topic": "Compare and contrast CREATE OR REPLACE TABLE and INSERT OVERWRITE",
        "answer": ""
      },
      {
        "topic": "Identify a scenario in which MERGE should be used.",
        "answer": ""
      },
      {
        "topic": "Identify MERGE as a command to deduplicate data upon writing.",
        "answer": ""
      },
      { "topic": "Describe the benefits of the MERGE command.", "answer": "" },
      {
        "topic": "Identify why a COPY INTO statement is not duplicating data in the target table.",
        "answer": ""
      },
      {
        "topic": "Identify a scenario in which COPY INTO should be used.",
        "answer": ""
      },
      { "topic": "Use COPY INTO to insert data.", "answer": "" },
      {
        "topic": "Identify the components necessary to create a new DLT pipeline.",
        "answer": ""
      },
      {
        "topic": "Identify the purpose of the target and of the notebook libraries in creating a pipeline.",
        "answer": ""
      },
      {
        "topic": "Compare and contrast triggered and continuous pipelines in terms of cost and latency",
        "answer": ""
      },
      {
        "topic": "Identify which source location is utilizing Auto Loader.",
        "answer": ""
      },
      {
        "topic": "Identify a scenario in which Auto Loader is beneficial.",
        "answer": ""
      },
      {
        "topic": "Identify why Auto Loader has inferred all data to be STRING from a JSON source",
        "answer": ""
      },
      {
        "topic": "Identify the default behavior of a constraint violation",
        "answer": ""
      },
      {
        "topic": "Identify the impact of ON VIOLATION DROP ROW and ON VIOLATION FAIL UPDATEfor a constraint violation",
        "answer": ""
      },
      {
        "topic": "Explain change data capture and the behavior of APPLY CHANGES INTO",
        "answer": ""
      },
      {
        "topic": "Query the events log to get metrics, perform audit loggin, examine lineage.",
        "answer": ""
      },
      {
        "topic": "Troubleshoot DLT syntax: Identify which notebook in a DLT pipeline produced an error, identify the need for LIVE in create statement, identify the need for STREAM in from clause.",
        "answer": ""
      }
    ]
  },
  {
    "section": 4,
    "title": "Production Pipelines",
    "topics": [
      {
        "topic": "Identify the benefits of using multiple tasks in Jobs.",
        "answer": ""
      },
      { "topic": "Set up a predecessor task in Jobs.", "answer": "" },
      {
        "topic": "Identify a scenario in which a predecessor task should be set up.",
        "answer": ""
      },
      { "topic": "Review a task's execution history.", "answer": "" },
      { "topic": "Identify CRON as a scheduling opportunity.", "answer": "" },
      { "topic": "Debug a failed task.", "answer": "" },
      { "topic": "Set up a retry policy in case of failure.", "answer": "" },
      {
        "topic": "Create an alert in the case of a failed task.",
        "answer": ""
      },
      { "topic": "Identify that an alert can be sent via email.", "answer": "" }
    ]
  },
  {
    "section": 5,
    "title": "Data Governance",
    "topics": [
      {
        "topic": "Identify one of the four areas of data governance.",
        "answer": ""
      },
      {
        "topic": "Compare and contrast metastores and catalogs.",
        "answer": ""
      },
      { "topic": "Identify Unity Catalog securables.", "answer": "" },
      { "topic": "Define a service principal.", "answer": "" },
      {
        "topic": "Identify the cluster security modes compatible with Unity Catalog.",
        "answer": ""
      },
      { "topic": "Create a UC-enabled all-purpose cluster.", "answer": "" },
      { "topic": "Create a DBSQL warehouse.", "answer": "" },
      {
        "topic": "Identify how to query a three-layer namespace.",
        "answer": ""
      },
      { "topic": "Implement data object access control", "answer": "" },
      {
        "topic": "Identify colocating metastores with a workspace as best practice.",
        "answer": ""
      },
      {
        "topic": "Identify using service principals for connections as best practice.",
        "answer": ""
      },
      {
        "topic": "Identify the segregation of business units across catalog as best practice.",
        "answer": ""
      }
    ]
  }
]
